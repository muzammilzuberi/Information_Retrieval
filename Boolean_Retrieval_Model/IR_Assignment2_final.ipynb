{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "path_to_zip_file = 'Abstracts.zip'    #Path to zip file\n",
        "directory_to_extract_to = 'Extracted'  #Directory name\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)           #extracting files"
      ],
      "metadata": {
        "id": "_xOaTbCtlH4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import codecs\n",
        "import string\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "docs_list = []\n",
        "doc_id_list=[]\n",
        "content=''                \n",
        "for filename in os.listdir('Extracted/Abstracts'):                      \n",
        "    if filename.endswith('.txt'):\n",
        "      pattern = '(\\d*).txt'                                               #removing .txt from file names using regular expression\n",
        "      docid=re.findall(pattern,filename)\n",
        "      doc_id = docid[0]                                                   \n",
        "      doc_id_list.append(doc_id)                             #storing doc ids in doc_list as files are not processed sequentially by os, so to sync later\n",
        "      with codecs.open(os.path.join('Extracted/Abstracts', filename), 'r', encoding='utf-8',errors='ignore') as fdata:\n",
        "        doc = fdata.read()                        #reading data \n",
        "        content= content+doc                      #to form dictionary\n",
        "        docs_list.append(doc)                      #to form posting lists"
      ],
      "metadata": {
        "id": "isdbnFoklKUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(content):\n",
        "  for char in string.punctuation:\n",
        "    content2 = content.replace(char,' ')                  #removing punctuations\n",
        "  tokens = content2.split()                               #splitting into tokens\n",
        "  tokens.sort()\n",
        "  tokens2 = [i.lower() for i in tokens]                   #case normalization\n",
        "  f = open('Stopword-List.txt')\n",
        "  stop_words = f.read()\n",
        "  stop_words_list = stop_words.split()\n",
        "  tokens3 = tokens2.copy()\n",
        "  tokens3.sort()\n",
        "  \n",
        "  #print(tokens3)\n",
        "\n",
        "  for i in stop_words_list:\n",
        "    if i in tokens3:\n",
        "      tokens3 = [x for x in tokens3 if i!=x]                            #removing stop words\n",
        "  tokens4 = tokens3.copy()\n",
        "  tokens4 = list(set(tokens3))                                          #removing duplicates\n",
        "  tokens4.sort()\n",
        "  tokens5 = tokens4.copy()\n",
        "  tokens6=[]\n",
        "  ps = PorterStemmer()\n",
        "  for i in tokens5:\n",
        "    tokens6.append(ps.stem(i))\n",
        "  \n",
        "  tokens7 = [i for i in tokens6 if not any(c.isdigit() for c in i)]           #removing digits\n",
        "  tokens7.sort()                                                              #sorting alphabetically \n",
        "  tokens8 = [i for i in tokens7 if len(i)>1]                                  #removing single character strings\n",
        "  #print(tokens7)\n",
        "  return tokens8"
      ],
      "metadata": {
        "id": "X3C417fFlPQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_doc(content):                          #same function as above except no sorting and no removing of duplicates for positional index \n",
        "  for char in string.punctuation:\n",
        "    content2 = content.replace(char,' ')\n",
        "  tokens = content2.split()\n",
        "  #tokens.sort()\n",
        "  tokens2 = [i.lower() for i in tokens]\n",
        "  f = open('Stopword-List.txt')\n",
        "  stop_words = f.read()\n",
        "  stop_words_list = stop_words.split()\n",
        "  tokens3 = tokens2.copy()\n",
        "  #tokens3.sort()\n",
        "  \n",
        "  #print(tokens3)\n",
        "\n",
        "  for i in stop_words_list:\n",
        "    if i in tokens3:\n",
        "      tokens3 = [x for x in tokens3 if i!=x]\n",
        "  tokens4 = tokens3.copy()\n",
        "  #tokens4 = list(set(tokens3))\n",
        "  #tokens4.sort()\n",
        "  tokens5 = tokens4.copy()\n",
        "  tokens6=[]\n",
        "  ps = PorterStemmer()\n",
        "  for i in tokens5:\n",
        "    tokens6.append(ps.stem(i))\n",
        "    tokens7 = [i for i in tokens6 if not any(c.isdigit() for c in i)]\n",
        "  #tokens7.sort()\n",
        "  tokens8 = [i for i in tokens7 if len(i)>1]\n",
        "  #print(tokens7)\n",
        "  return tokens8"
      ],
      "metadata": {
        "id": "s2Nwi6Q4lRiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_tokens=[]                                     #maintaining list of doc_tokens for indexes      \n",
        "for doc in docs_list:\n",
        "  tok = tokenize_doc(doc)\n",
        "  doc_tokens.append(tok)"
      ],
      "metadata": {
        "id": "1VBpeuF8lTFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(content):                #seprate removing pucntuation to remove punctuation from whole content\n",
        "  content2=content\n",
        "  for char in string.punctuation:\n",
        "    content2 = content2.replace(char,' ')\n",
        "  return content2"
      ],
      "metadata": {
        "id": "GbSTJKGSlXB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_ret= remove_punctuation(content)         \n",
        "tokens = tokenize(content_ret)                  #tokenizing the content (whole collection string)\n",
        "for token in tokens[0:10]:                      #checking first few tokens\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8xDU2O4nLM1",
        "outputId": "270a7419-c4dc-4b4f-9921-2739f856b76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aal\n",
            "aapl\n",
            "abbrevi\n",
            "abc\n",
            "abdomin\n",
            "abduct\n",
            "abil\n",
            "abil\n",
            "abl\n",
            "abnorm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = {k:[] for k in tokens}    #adding keys to dictionary"
      ],
      "metadata": {
        "id": "xhF2jmtjnbaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:                  #adding posting list to dictionary\n",
        "  i=0                                 #used to sync the document ids\n",
        "  for doc_token in doc_tokens:\n",
        "    if(token in doc_token):\n",
        "       d[token].append(doc_id_list[i])\n",
        "    i=i+1"
      ],
      "metadata": {
        "id": "6kmLSVVdnfJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp=d.copy()\n",
        "for key,value in temp.items():       #removing duplicates from dictionary \n",
        "  if(len(value)==0):\n",
        "    del d[key]"
      ],
      "metadata": {
        "id": "xampRqPDqivC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k,v in d.items():       #removing duplicates from docs and sorting them in ascending order (duplicates because same list for positional and inverted)\n",
        "  v = list(set(v))\n",
        "  v.sort()\n",
        "  d[k]=v"
      ],
      "metadata": {
        "id": "QamMzJzRqsB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"inverted_index2.json\", \"w\") as outfile:      #storing inversted index\n",
        "    json.dump(d, outfile)"
      ],
      "metadata": {
        "id": "feAF1DePq9d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi = {k:[] for k in tokens}      #adding keys to positional index"
      ],
      "metadata": {
        "id": "7OW5nFCCrR8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:                    #creating positional index\n",
        "  i=0\n",
        "  for doc_token in doc_tokens:\n",
        "    if(token in doc_token):                     #checking the occurance in document\n",
        "       doc_id= doc_id_list[i]                   #getting doc id same as inverted index\n",
        "       pos=doc_tokens[i].index(token)            #getting position by fetching index of token in tokenized documents\n",
        "       x = (doc_id,pos)                     #doc id, position tuple\n",
        "       pi[token].append(x)                       #appending tuples into the index           \n",
        "    i=i+1"
      ],
      "metadata": {
        "id": "bL8ob19Brbdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp=pi.copy()                      #removing duplicates from dictionary\n",
        "for key,value in temp.items():\n",
        "  if(len(value)==0):\n",
        "    del pi[key]"
      ],
      "metadata": {
        "id": "JgLhLo06s8cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"positional_index2.json\", \"w\") as outfile:        #storing positional index\n",
        "    json.dump(pi, outfile)"
      ],
      "metadata": {
        "id": "xRwjJ03ItKx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def proximity_queries(query_terms,distance,pi,d):\n",
        "                                                      \n",
        "  result_set= []\n",
        "  retrieved_lists = []\n",
        "  j=0\n",
        "  ps= PorterStemmer()\n",
        "  qt=[]\n",
        "  for i in query_terms:\n",
        "    i = ps.stem(i)\n",
        "    qt.append(i)\n",
        "\n",
        "  rs={k:[] for k in qt}\n",
        "  rs2={k:[] for k in qt}\n",
        "  for i in qt:\n",
        "    rs[i].append(pi[i])\n",
        "    retrieved_lists.append(d[i])\n",
        "  result_set = retrieved_lists[0]\n",
        "\n",
        "  for i in range(len(query_terms)-1):\n",
        "    result_set = list(set(result_set) & set(retrieved_lists[i+1]))\n",
        "\n",
        "  x = ()\n",
        "  for k,v in rs.items():\n",
        "    for i in v:\n",
        "      for j in i:\n",
        "        for m in result_set:\n",
        "          if(m==j[0]):\n",
        "            rs2[k].append((j[0],j[1]))\n",
        "  \n",
        "\n",
        "  for k,v in rs2.items():    \n",
        "    v = list(set(v))\n",
        "    v.sort()\n",
        "    rs2[k]=v\n",
        "\n",
        "  n = len(rs2[qt[0]])\n",
        "  n2 = len(rs2)\n",
        "\n",
        "\n",
        "  final_set=[]\n",
        "  for i in range(0,n):\n",
        "    dist=[]\n",
        "    for j in range(0,n2):\n",
        "      dist.append(rs2[qt[j]][i][1])\n",
        "    for j in range(0,n2):\n",
        "      if(abs(dist[0]-dist[1])<=distance):\n",
        "        final_set.append(rs2[qt[j]][i][0])\n",
        "  final_set = list(set(final_set))\n",
        "  return final_set"
      ],
      "metadata": {
        "id": "O4o0d44Hvin1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_terms = ['neural','information']\n",
        "proximity_queries(query_terms,3,pi,d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhfppZkbxdu-",
        "outputId": "a914b341-6f3a-4af7-c84e-408d8deddc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['26']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_boolean_query(query_terms,d,operations):       #function to resolve simple boolean query\n",
        "  stemmed_query=[]                                       \n",
        "  retrieved_lists = []                    \n",
        "  ps= PorterStemmer()\n",
        "  for i in query_terms:\n",
        "    if i[-1]=='!':\n",
        "      temp=i\n",
        "      i=i[:-1]\n",
        "      s=ps.stem(i)\n",
        "      rl= d[s]\n",
        "      v = [str(i) for i in range(1,448) if str(i) not in rl]     #taking compliment if term is with NOT\n",
        "      retrieved_lists.append(v)\n",
        "      continue\n",
        "    s=ps.stem(i)   \n",
        "                                        #stemming the query\n",
        "    retrieved_lists.append(d[s])                    #adding doc ids in retrieved list\n",
        "  result_set = retrieved_lists[0]                   #adding first posting in reseult set\n",
        "  j=1\n",
        "  for i in operations:\n",
        "    if i==0:\n",
        "      result_set = list(set(result_set) & set(retrieved_lists[j]))      #checking if operation is and/or and applying te operation with next posting\n",
        "    elif i==1:\n",
        "      result_set = list(set(set(result_set) | set(retrieved_lists[j])))\n",
        "\n",
        "    j=j+1\n",
        "  return (result_set)"
      ],
      "metadata": {
        "id": "H726GZLJkoha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_preprocessing(query,d):\n",
        "  breaking = query.split()         \n",
        "  query_terms=[]\n",
        "  operations = []\n",
        "  flag=False\n",
        "  for i in breaking:\n",
        "    if(i=='AND'):\n",
        "      operations.append(0)              #using operations list to specify the operation\n",
        "    elif (i=='OR'):\n",
        "      operations.append(1)\n",
        "    elif (i=='NOT'):    \n",
        "      flag=True\n",
        "    else:\n",
        "      if flag==True:\n",
        "        i = i+'!'                        #adding ! to mark it as NOT to compliment later\n",
        "      flag=False\n",
        "      query_terms.append(i)\n",
        "  rs=simple_boolean_query(query_terms,d,operations)\n",
        "  #rs.sort()\n",
        "\n",
        "  return rs"
      ],
      "metadata": {
        "id": "4-WHHnVECzlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'image AND NOT learning AND machine'\n",
        "query_preprocessing(query,d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJa93zRMMtyI",
        "outputId": "11bc0d89-acea-4f13-ea5e-b14833331b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['371', '102', '126', '125']"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def proximity_queries_processing(query,d,pi):\n",
        "  query_terms = []\n",
        "  pattern= '\\d'\n",
        "  match=re.findall(pattern,query)\n",
        "  distance = int(match[0])\n",
        "  for i in query.split():\n",
        "   query_terms.append(i)\n",
        "  query_terms.pop()\n",
        "  return (proximity_queries(query_terms,distance,pi,d))"
      ],
      "metadata": {
        "id": "J5JcNi3aMybv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'feature track /5'\n",
        "print(proximity_queries_processing(query,d,pi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0BHdjQyPW0q",
        "outputId": "6d965958-182b-45d0-a474-db44f6126e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['212', '13']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def phrase_query(phrase,d,pi):\n",
        "  terms = phrase.split()\n",
        "  n=len(terms)\n",
        "  operations=[]\n",
        "  for i in range(n-1):\n",
        "    operations.append(0)\n",
        "  rs = simple_boolean_query(terms,d,operations)       #initially applying simple boolean model\n",
        "  \n",
        "  if(len(rs)>10):                                     #if result set more than 10 then using proximity query\n",
        "    rs=proximity_queries(terms,10,pi,d)\n",
        "  return rs"
      ],
      "metadata": {
        "id": "ow0GlVudteYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_query(query,d,pi):\n",
        "  if('NOT' in query or 'AND' in query or 'OR' in query):\n",
        "    print(query_preprocessing(query,d))\n",
        "\n",
        "  elif query[-1].isdigit():\n",
        "    print(proximity_queries_processing(query,d,pi))\n",
        "\n",
        "  else:\n",
        "    print(phrase_query(query,d,pi))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eHDgjPvphuy",
        "outputId": "96d38210-c55c-4cf7-c0db-ecfba4cc45eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['212', '13']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'image AND restoration'\n",
        "classify_query(query,d,pi)"
      ],
      "metadata": {
        "id": "B_rsKd9Z3aRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f625c677-8deb-4b6e-f639-768034535e3d"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['359', '375']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'time AND series OR classification'\n",
        "classify_query(query,d,pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydQGFXw19QdN",
        "outputId": "3dd4f330-6647-47f0-9a4e-33243bd760f9"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['75', '107', '9', '438', '66', '445', '280', '338', '384', '234', '51', '46', '235', '158', '286', '302', '256', '77', '6', '173', '277', '84', '259', '182', '377', '261', '442', '295', '345', '341', '167', '283', '353', '140', '60', '55', '193', '94', '371', '128', '40', '56', '175', '120', '198', '33', '10', '317', '354', '375', '143', '95', '123', '305', '125', '238', '71', '421', '228', '174', '239', '395', '245', '252', '220', '126', '171', '357', '64', '272', '397', '287', '24', '215', '334', '164', '22', '386', '58', '85', '249', '247', '176', '122', '59', '268', '369', '99', '38', '208', '303', '76', '210', '427', '321', '165', '177', '439', '240', '404', '111', '385', '432', '113', '187', '405', '236', '237', '348', '299', '328', '45', '425', '202', '378', '265', '350', '168', '16', '273', '424', '63', '34', '352', '4', '313', '73', '43', '316', '387', '54', '229', '67', '197', '106']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'feature track /5'\n",
        "classify_query(query,d,pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic8dZW479XAk",
        "outputId": "53c100d8-fdd2-4f4c-c7f5-af7e739c8c18"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['212', '13']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=input()\n",
        "classify_query(query,d,pi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BD1LoC1-Ejk",
        "outputId": "69da726b-6288-4c37-e79f-7307e2974e3a"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image processing\n",
            "['247', '162', '366', '358', '352']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zFElp2J4-JsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}